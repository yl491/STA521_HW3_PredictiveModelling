---
title: "HW3 [Team 12]"
author: '[Yunxuan Li, Wenxin Liao, Yan Zhao]'
date: "Due September 27, 2017"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(foreign)
library(magrittr)
library(plyr)
library(dplyr)
library(ggplot2)
library(knitr)
# add other libraries
```


We will explore logistic regression with the National Election Study data from Gelman & Hill (GH).  (See Chapter 4.7 for descriptions of some of the variables and 5.1 of GH for initial model fitting).  The link here may also be useful for background information http://gking.harvard.edu/files/preelection.pdf

[*The following code will read in the data and perform some filtering/recoding. Remove this text and modify the  code chunk options so that the code does not appear in the output.*]

```{r data}
# Data are also at http://www.stat.columbia.edu/~gelman/arm/examples/nes

nes <- read.dta("nes5200_processed_voters_realideo.dta",
                   convert.factors=F)
# Data cleaning
# remove NA's for key variables first
nes1992 = nes %>% filter(!is.na(black)) %>%
              filter(!is.na(female)) %>%
              filter(!is.na(educ1)) %>%
              filter(!is.na(age)) %>%
              filter(!is.na(state)) %>%
              filter(!is.na(income)) %>%
              filter(presvote %in% 1:2) %>% 
# limit to year 1992 t0 2000 and add new varialbes
              filter(year == 1992) %>%
              mutate(female = gender -1,
                     black=race ==2,
# recode vote so that vote = 1 corresponds to a vote for Bush,
# and vote=0 is a vote for Clinton, where votes for Ross Perot were 
# removed earlier                     
                     vote = presvote == 2)
```

1. Summarize the data for 1992 noting which variables have missing data.  Which variables are categorical but are coded as numerically? \


Answer: "gender", "race", "education", "prevote"ï¼Œurban, region, income, occup, union, religion, martial status, partyid, fatherparty, motherparty, dislikes, rlikes are all categorical but are coded as numerically.
```{r}
# add code in this chunk and click green arrow on right to run or use
# the Run menu.   Add additional chunks using the Insert menu or 
# short-cut keys.  

summary(nes1992)
sum(is.na(nes1992$occup1))
c<-rep(0,62)
for (i in 1:length(nes1992))
{
  c[i]=sum(is.na(nes1992[,i]))
}
names(c)=colnames(nes1992)
c[c>0]="Missing"
c[c==0]="Good"
cc<-as.matrix(c)

```
\

2. Fit the logistic regression  to estimate the probability that an individual would vote Bush (Republican) as a function of income and provide a summary of the model.
```{r}
vote.glm=glm(formula=vote~income,data=nes1992,family=binomial())
summary(vote.glm)

```


3. Obtain a point estimate and create a 95% confidence interval for the odds ratio for voting Republican for a rich person (income category 5) compared to a poor person (income category 1). *Hint this is more than a one unit change; calculate manually and then show how to modify the output from confint*. Provide a sentence interpreting the result.
```{r}

oddsratio=exp(0.32599*(5-1))
oddsratio
CIhigh=0.32599+1.96*0.05688
CIlow=0.32599-1.96*0.05688
exp(CIhigh*4)
exp(CIlow*4)

exp(4*confint(vote.glm,"income"))
```
For rich, the odds of voting for republican is 3.68 larger than the odds of poor people voting for the republican.

4.  Obtain fitted probabilities and 95% confidence intervals for the income categories using the `predict` function.  Use `ggplot` to recreate the plots in figure 5.1 of Gelman & Hill.  *write a general function?*

```{r}


vote.pre=predict(vote.glm,data=nes1992,data.frame(income=c(1,2,3,4,5)),type='response',interval=c("confidence"),level=0.95)
vote.pre
#use the version below#
#nes1992$vote.1 = as.double(nes1992$vote)
#ggplot()+geom_point(data=nes1992,aes(income,vote.1),position="jitter")+geom_smooth(data=nes1992,method="glm",method.args=list(family="binomial"),aes(income,vote.1),col="red",se=T)
#end#
#use the version below- Yunxuan""
#first plot#
ggplot(data=nes1992,aes(x=income,y=as.numeric(vote)))+geom_point(position=position_jitter(width=0.3,height=0.03),shape=21,size=0.01)+geom_smooth(data=nes1992,method="glm",aes(x=income,y=as.numeric(vote)),method.args=list(family="binomial"),fullrange=T,se=F,size=0.5)+scale_x_continuous(limits=c(-5,10))+geom_smooth(data=nes1992,method="glm",aes(x=income,y=as.numeric(vote)),method.args=list(family="binomial"),size=1,se=F)

#second plot#
ggplot(data=nes1992,aes(x=income,y=as.numeric(vote)))+geom_point(position=position_jitter(width=0.3,height=0.1),shape=21,size=0.01)+geom_smooth(data=nes1992,method="glm",method.args=list(family="binomial"),aes(x=income,y=as.numeric(vote)),fullrange=T,size=0.5)



```

5.  What does the residual deviance or any diagnostic plots suggest about the model?  (provide code for p-values and output and plots) 
```{r}

vote.glm0=glm(vote~1,data=nes1992,family=binomial)
anova(vote.glm0,vote.glm,test="Chi")
plot(vote.glm)

```
\
Answer: Figure 3 suggests underfitting.\
6. Create a new data set by the filtering and mutate steps above, but now include years between 1952 and 2000.
```{r}


# Data cleaning
# remove NA's for key variables first
nes19522000 = nes %>% filter(!is.na(black)) %>%
              filter(!is.na(female)) %>%
              filter(!is.na(educ1)) %>%
              filter(!is.na(age)) %>%
              filter(!is.na(state)) %>%
              filter(!is.na(income)) %>%
              filter(presvote %in% 1:2) %>% 
              filter(year>1951 | year<2001) %>%
              mutate(female = gender -1,
                     black=race ==2,
# recode vote so that vote = 1 corresponds to a vote for Bush,
# and vote=0 is a vote for Clinton, where votes for Ross Perot were 
# removed earlier                     
                     vote = presvote == 2)
```

7. Fit a separate logistic regression for each year from 1952 to 2000, using the `subset` option in `glm`,  i.e. add `subset=year==1952`.  For each find the 95% Confidence interval for the odds ratio of voting republican for rich compared to poor for each year in the data set from 1952 to 2000.
```{r}

vote.glm.1952<-glm(formula=vote~income,data=nes19522000,subset=year==1952,family=binomial())
vote.glm.1956<-glm(formula=vote~income,data=nes19522000,subset=year==1956,family=binomial())
vote.glm.1960<-glm(formula=vote~income,data=nes19522000,subset=year==1960,family=binomial())
vote.glm.1964<-glm(formula=vote~income,data=nes19522000,subset=year==1964,family=binomial())
vote.glm.1968<-glm(formula=vote~income,data=nes19522000,subset=year==1968,family=binomial())
vote.glm.1972<-glm(formula=vote~income,data=nes19522000,subset=year==1972,family=binomial())
vote.glm.1976<-glm(formula=vote~income,data=nes19522000,subset=year==1976,family=binomial())
vote.glm.1980<-glm(formula=vote~income,data=nes19522000,subset=year==1980,family=binomial())
vote.glm.1984<-glm(formula=vote~income,data=nes19522000,subset=year==1984,family=binomial())
vote.glm.1988<-glm(formula=vote~income,data=nes19522000,subset=year==1988,family=binomial())
vote.glm.1992<-glm(formula=vote~income,data=nes19522000,subset=year==1992,family=binomial())
vote.glm.1996<-glm(formula=vote~income,data=nes19522000,subset=year==1996,family=binomial())
vote.glm.2000<-glm(formula=vote~income,data=nes19522000,subset=year==2000,family=binomial())

conf_int<-matrix(data=NA, nrow=13, ncol=3)

conf_int[1,]<-cbind(exp(coefficients(summary(vote.glm.1952))[2,1]*4),exp(4*confint(vote.glm.1952,"income"))[1],exp(4*confint(vote.glm.1952,"income"))[2])
conf_int[2,]<-cbind(exp(coefficients(summary(vote.glm.1956))[2,1]*4),exp(4*confint(vote.glm.1956,"income"))[1],exp(4*confint(vote.glm.1956,"income"))[2])
conf_int[3,]<-cbind(exp(coefficients(summary(vote.glm.1960))[2,1]*4),exp(4*confint(vote.glm.1960,"income"))[1],exp(4*confint(vote.glm.1960,"income"))[2])
conf_int[4,]<-cbind(exp(coefficients(summary(vote.glm.1964))[2,1]*4),exp(4*confint(vote.glm.1964,"income"))[1],exp(4*confint(vote.glm.1964,"income"))[2])
conf_int[5,]<-cbind(exp(coefficients(summary(vote.glm.1968))[2,1]*4),exp(4*confint(vote.glm.1968,"income"))[1],exp(4*confint(vote.glm.1968,"income"))[2])
conf_int[6,]<-cbind(exp(coefficients(summary(vote.glm.1972))[2,1]*4),exp(4*confint(vote.glm.1972,"income"))[1],exp(4*confint(vote.glm.1972,"income"))[2])
conf_int[7,]<-cbind(exp(coefficients(summary(vote.glm.1976))[2,1]*4),exp(4*confint(vote.glm.1976,"income"))[1],exp(4*confint(vote.glm.1976,"income"))[2])
conf_int[8,]<-cbind(exp(coefficients(summary(vote.glm.1980))[2,1]*4),exp(4*confint(vote.glm.1980,"income"))[1],exp(4*confint(vote.glm.1980,"income"))[2])
conf_int[9,]<-cbind(exp(coefficients(summary(vote.glm.1984))[2,1]*4),exp(4*confint(vote.glm.1984,"income"))[1],exp(4*confint(vote.glm.1984,"income"))[2])
conf_int[10,]<-cbind(exp(coefficients(summary(vote.glm.1988))[2,1]*4),exp(4*confint(vote.glm.1988,"income"))[1],exp(4*confint(vote.glm.1988,"income"))[2])
conf_int[11,]<-cbind(exp(coefficients(summary(vote.glm.1992))[2,1]*4),exp(4*confint(vote.glm.1992,"income"))[1],exp(4*confint(vote.glm.1992,"income"))[2])
conf_int[12,]<-cbind(exp(coefficients(summary(vote.glm.1996))[2,1]*4),exp(4*confint(vote.glm.1996,"income"))[1],exp(4*confint(vote.glm.1996,"income"))[2])
conf_int[13,]<-cbind(exp(coefficients(summary(vote.glm.2000))[2,1]*4),exp(4*confint(vote.glm.2000,"income"))[1],exp(4*confint(vote.glm.2000,"income"))[2])

colnames(conf_int)=cbind("Value","lower","upper")
rownames(conf_int)=c("1952","1956","1960","1964","1968","1972","1976","1980","1984","1988","1992","1996","2000")

#conf_int
knitr::kable(conf_int)



conf_int_df<-as.data.frame(conf_int)



```
8.  Using `ggplot` plot the confidence intervals over time similar to the display in Figure 5.4.
```{r}
ggplot(conf_int_df,aes(x=rownames(conf_int_df),y=Value))+geom_point()+geom_linerange(aes(ymin=lower,ymax=upper))+xlab("Year")+ylab("CI")
```

9. Fit a logistic regression using income and year as a factor  with an interaction i.e. `income*factor(year)` to the data from 1952-2000.  Find the log odds ratio for income for each year by combining parameter estimates and show that these are the same as in the respective individual logistic regression models fit separately to the data for each year.
```{r}
vote.glm_inte<-glm(formula=vote~income*factor(year),data=nes19522000,family=binomial())
#summary(vote.glm_inte)
su<-coefficients(summary(vote.glm_inte))
oddsr<-numeric(13)
oddsr[1]=exp(su[2,1]*4)
for (i in 1: 12)
{
  oddsr[i+1]=exp(4*(su[2,1]+su[14+i,1]))
}
oddsr

```

\
Answer: Comparing this with the table in problem 7, we can see the numbers of oddsratio are the same.\
10.  Create a plot of fitted probabilities and confidence intervals as in question 4, with curves for all years in the same plot. 
```{r}
#shape,size,alpha in geom point
ggplot(data=nes19522000,aes(x=income,y=as.numeric(vote),group=factor(year)))+geom_point(position=position_jitter(width=0.3,height=0.1),shape=21,size=0.01)+geom_smooth(data=nes19522000,method="glm",method.args=list(family="binomial"),aes(x=income,y=as.numeric(vote),group=factor(year),color=factor(year)),fullrange=T)+scale_x_continuous(limits=c(-3,8))
```


11.  Return to the 1992 year data. Filter out rows of `nes1992` with NA's in the variables below and  recode as factors using the levels in parentheses:
    + gender (1 = "male", 2 = "female"), 
    + race (1 = "white", 2 = "black", 3 = "asian", 4 = "native american", 5 = "hispanic", 7 = "other"), 
    + education ( use `educ1` with levels 1 = "no high school", 2 = "high school graduate", 3 = "some college", 4 = "college graduate"), 
    + party identification (`partyid3` with levels 1= "democrats", 2 = "independents", 3 = "republicans", 9 = "apolitical" , and 
    + political ideology (`ideo` 1 = "liberal", 3 ="moderate", 5 = "conservative") 
```{r}


nes1992filter = nes1992 %>% filter(!is.na(gender)) %>%
                   filter(!is.na(race)) %>%
                   filter(!is.na(educ1)) %>%
                   filter(!is.na(partyid3)) %>%
                   filter(!is.na(ideo))
nes1992filter$gender<-mapvalues(nes1992filter$gender, from=c("1","2"), to =c("male","female"))
nes1992filter$race<-mapvalues(nes1992filter$race, from=c("1","2","3","4","5","7"), to =c("white","black","asian","native american","hispanic","other"))
nes1992filter$education<-mapvalues(nes1992filter$educ1, from=c("1","2","3","4"), to =c("no high school","high school graduate","some college","college graduate"))
nes1992filter$party_id<-mapvalues(nes1992filter$partyid3, from=c("1","2","3","9"), to =c("democrats","independents","republicans","apolitical"))
nes1992filter$poli_id<-mapvalues(nes1992filter$ideo, from=c("1","3","5"), to =c("liberal","moderate","conservative"))

                          
```

12.  Fit a logistic regression model predicting support for Bush given the the variables above and income as predictors and also consider interactions among the predictors.   You do not need to consider all possible interactions or use model selection, but suggest a couple from the predictors above that might make sense intuitively. 
```{r}
#glm3<-glm(formula=vote~gender+race+education+party_id+poli_id
         # +race:education+race:party_id+race:poli_id
        #  +gender:education+gender:poli_id+gender:party_id
        #  +education:party_id+education:poli_id
        #  +party_id:poli_id, data=nes1992filter,family="binomial")
#glm3<-glm(formula=vote~gender:party_id+education,data=nes1992filter,family="binomial")
#the best i could ever find after testing for 1.5 hours: 1072.9 vs 1113 â†“
glm3<-glm(formula=vote~poli_id+education:race,data=nes1992filter,family="binomial")
summary(glm3)
```
\
Answer: We think political identification has an influence on vote; also, race may have an interaction with education.\
13.  Plot binned residuals using the function `binnedplot` from package `arm` versus some of the additional predictors in the 1992 dataframe.  Are there any suggestions that the mean or distribution of residuals is different across the levels of the other predictors and that these predictors should be added to the model?  (Provide plots and any other summaries to explain).   
```{r}
#predict<-predict(fit,type="response")
#table(y,predict>0.5)
binnedplot(nes1992filter$age, rstandard(glm3))
binnedplot(nes1992filter$state,rstandard(glm3))
binnedplot(na.omit(nes1992filter$dlikes),rstandard(glm3))
```
\
Answer: For variabl age and variable state, most of the points lie in the 95% CI, and I don't see any obvious patterns. However, there is a pattern in the plot for variable dlikes, which suggests it may need to be inluded to the model.\

14.  Evaluate and compare the different models you fit.  Consider coefficient estimates (are they stable across models) and standard errors (any indications of identifiability problems), residual plots and deviances.\
Answer1: We compare the w/ and w/o the interaction, as is mentioned by the professor in Piazza.\
```{r}
#glm3
summary(glm3)

#without interaction
glm_sole<-glm(formula=vote~poli_id,family="binomial",data=nes1992filter)
summary(glm_sole)


```
\
Answer2: With interaction vs without interaction: with interaction, res: 1072, dof: 1113, only a little bit overfitting; without interaction, res: 1202, 1130, underfitting. We have more information and is better fitting the data. The coefficients for "political identification" are -2.91, -1.07 in the glm3(the one w/ interaction), and -2.69, -1.16 in the model w/o interaction -- I believe they are pretty consistent across models. Standard errors look good to me: they are really small. Also, many coefficients in the glm3 model(the one w/ interaction) are significant, which means that the interaction between race and education is sth that we needs to take into account.
\
```{r}
plot(glm3)
plot(glm_sole)

```
\
Answer: the Q-Q plot of glm3 is much better! the left part almost connects to the right part, and within each part it is much more continuous; the Q-Q plot of glm_sole is much more discrete and disconnected. Also if we look at the residual vs leverage plots, the one of glm3 does not have influential points, but the one of glm_sole has. To summarize, the model with interaction performs better and fits better! \
15.  Compute the error rate of your model (see GH page 99) and compare it to the error rate of the null model.  We can define a function for the error rate as:
```{r}
error.rate = function(pred, true) {
  mean((pred > .5 & true == 0) | (pred < .5 & true == 1))
}
glm.probs=predict(glm3,type='response')
glm.null<-glm(vote~1,data=nes1992filter,family = "binomial")
predictnull=predict(glm.null,type="response")
table(nes1992filter$vote,glm.probs>0.5)
1-(509+380)/(509+159+85+380)
table(nes1992filter$vote,predictnull>0.5)
1-668/(668+465)

```
\
Answer: As the error rate for null is 0.412 and the error rate for our model is 0.215,our model perform better
\
16.  For your chosen model, discuss and compare the importance of each input variable in the prediction.   Provide a neatly formatted table of odds ratios  and 95\% confidence intervals.
```{r}
table<-matrix(NA,20,3)
j=1 
for(i in rownames(coefficients(summary(glm3)))){
  table[j,1]<-exp((summary(glm3)$coefficients[j,1])+(summary(glm3)$coefficients[j,2])*1.96)
  table[j,2]<-exp((summary(glm3)$coefficients[j,1])-(summary(glm3)$coefficients[j,2])*1.96)
  table[j,3]<-exp(summary(glm3)$coefficients[j,1])
  j=j+1
}
colnames(table)<-c("97.5%CI","2.5%CI", "odds ratio")
rownames(table)<-rownames(coefficients(summary(glm3)))

kable(table)
```
\
Answer: According to the p value of the summary of our chosen model, five predictors has very low p value, which means we reject null and those are very significant in predicting our response variable.Those predictors are poli_id liberal, poli_id moderate,interaction of education college graduate with race of black, interaction of education high school graduate with race of balck and interaction of education some college and race of black. In a word the input variables poli_id and interaction of education and race are all very important.\

17.  Provide a paragraph summarizing your findings and interpreting key coefficients (providing ranges of supporting values from above) in terms of the odds of voting for Bush.  Attempt to write this at a level that readers of the New York Times Upshot column could understand.   
\
We have identify five important coefficients in the previous problem.To interprete those key coeeffcients, we have following conclusion.
When a person identify its poli_id as liberal, we expect the odds of voting for bush decrease.
When a person identify its poli_id as moderate, we expect the odds of voting for bush will be  smaller
When a person has  education in college, with race of black, we expect the odds of voting for bush will decrease.
When a person has  education in high school, with race of black, we expect the odds of voting for bush will decrease.
When a person has  education in some college with race of black, we expect the odds of voting for bush will decrease.\

18.  In the above analysis, we removed missing data.  Repeat the data cleaning steps, but remove only the rows where the response variable, `presvote` is missing.  Recode all of the predictors (including income) so that there is a level that is 'missing' for any NA's for each variable.  How many observations are there now compared to the complete data?
```{r}
nes1992new <- nes %>%
  filter(!is.na(presvote)) %>%
              filter(presvote %in% 1:2) %>% 
              filter(year == 1992) %>%
              mutate(female = gender -1,
                     black=race == 2,
                     vote = presvote == 2) %>%

  
          dplyr::select(income, race, educ1, partyid3, ideo, vote)

nes1992new$poli_id<-mapvalues(nes1992new$ideo, from=c("1","3","5"), to =c("liberal","moderate","conservative"))
nes1992new$gender<-mapvalues(nes1992new$gender, from=c("1","2"), to =c("male","female"))
nes1992new$race<-mapvalues(nes1992new$race, from=c("1","2","3","4","5","7"), to =c("white","black","asian","native american","hispanic","other"))
nes1992new$education<-mapvalues(nes1992new$educ1, from=c("1","2","3","4"), to =c("no high school","high school graduate","some college","college graduate"))



names(nes1992new)[-6]

for (column in names(nes1992new)[-6]){
  nes1992new[is.na(nes1992new[, column]), column] <- "missing"
  nes1992new[, column] <- as.factor(nes1992new[, column])
}
summary(nes1992new)
```
\
Answer: Now we have 1304 observations.\

19. For any of above variables, suggest possible reasons why they may be missing.
\
We can see that there are few missing data for partyid. It's because once people choose a party, it's very likely for the person to persist. Because education changed constantly and is difficult to record, there are more missing data for education. 
\

20.  Rerun your selected model and create a table of parameter estimates and confidence intervals for the odds ratios.  You should have an additional coefficient for any categorical variable with missing data.   Comment on any changes in results for the model including the missing data and the previous one that used only complete data.
```{r}

glm4<-glm(formula=vote~poli_id+education:race,data=nes1992new,family="binomial")
summary(glm4)


table<-matrix(NA,30,3)
j=1 
for(i in rownames(coefficients(summary(glm4)))){
  table[j,1]<-exp((summary(glm4)$coefficients[j,1])+(summary(glm4)$coefficients[j,2])*1.96)
  table[j,2]<-exp((summary(glm4)$coefficients[j,1])-(summary(glm4)$coefficients[j,2])*1.96)
  table[j,3]<-exp(summary(glm4)$coefficients[j,1])
  j=j+1
}
colnames(table)<-c("97.5%CI","2.5%CI", "odds ratio")
rownames(table)<-rownames(coefficients(summary(glm4)))

kable(table)


```




